{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGvdqNXbnl-U",
        "outputId": "4e09febd-5525-4221-f404-5d9559c76dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 'Body' keys: 12294\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Replace 'Emails.json' with the name of your JSON file\n",
        "json_file = 'Emails.json'\n",
        "\n",
        "# Read the JSON file\n",
        "with open(json_file, mode='r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Count the number of 'Body' keys\n",
        "body_count = sum(1 for row in data if 'Body' in row)\n",
        "\n",
        "print(f\"Number of 'Body' keys: {body_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lowercasing to the 'Body' field\n",
        "for row in data:\n",
        "    if 'Body' in row:\n",
        "        row['Body'] = row['Body'].lower()"
      ],
      "metadata": {
        "id": "jKFsOkLXoshZ"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to remove email headers\n",
        "def remove_email_headers(text):\n",
        "    # The regex pattern below matches common email headers\n",
        "    pattern = r\"(?:From|To|Subject|Date|Cc|Bcc):\\s*[^\\n]*\\n\"\n",
        "    return re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "# Apply the remove_email_headers function to each email body\n",
        "for row in data:\n",
        "    if 'Body' in row:\n",
        "        row['Body'] = remove_email_headers(row['Body'])"
      ],
      "metadata": {
        "id": "hCig7vFio4ef"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return url_pattern.sub('', text)\n",
        "\n",
        "# Remove URLs from the 'Body' field\n",
        "for row in data:\n",
        "    if 'Body' in row:\n",
        "        row['Body'] = remove_urls(row['Body'])"
      ],
      "metadata": {
        "id": "c0t6-3tupTEm"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 -q"
      ],
      "metadata": {
        "id": "6YEp336-pfZM"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Remove HTML tags from the 'Body' field\n",
        "for row in data:\n",
        "    if 'Body' in row:\n",
        "        soup = BeautifulSoup(row['Body'], 'html.parser')\n",
        "        row['Body'] = soup.get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS2ZVmNZpwZ5",
        "outputId": "037448c8-ad4f-4a9f-ea15-4fdf9a070744"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-239-e640f458e775>:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(row['Body'], 'html.parser')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to remove non-alphabetic characters\n",
        "def remove_non_alphabetic(text):\n",
        "    # Replace non-alphabetic characters with spaces\n",
        "    return re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "# Iterate through the list of dictionaries and clean the 'Body' key\n",
        "for row in data:\n",
        "    if 'Body' in row:\n",
        "        row['Body'] = remove_non_alphabetic(row['Body'])"
      ],
      "metadata": {
        "id": "Xq162dYdp53H"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the 'Body' value of the first three rows\n",
        "for i, row in enumerate(data):\n",
        "    if i < 3:\n",
        "        print(f\"Row {i+1}:\")\n",
        "        print(row['Body'])\n",
        "        print()\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ4OY4bIqJLu",
        "outputId": "5eb7ff6c-074a-4503-d69a-4c840f60dd07"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1:\n",
            "you re signed up to receive a daily report of some notifications from your canvas account  below is the report for    mar \n",
            "\n",
            "submission comment  achraf ajrhourh             project    sp   csc       data engineering and visualization\n",
            "\n",
            "tajjeeddine rachidi just made a new comment on the submission for achraf ajrhourh            for project    \n",
            "\n",
            "click to view   \n",
            "\n",
            "   \t\n",
            "update your notification settings    \t\n",
            "\n",
            "\n",
            "Row 2:\n",
            "dear community  \n",
            "\n",
            "we would like to remind you that we will go back from gmt   to gmt  that is  time on watches should be decreased by one hour starting sunday   th at  am \n",
            "\n",
            "we would also like to take this opportunity to wish you a blessed ramadan  may this holy month bring you prosperity  health  and peace \n",
            "\n",
            "ramadan mubarak kareem\n",
            "\n",
            "best regards \n",
            "\n",
            "\n",
            "\n",
            "student government association\n",
            "\n",
            "\n",
            "al akhawayn university\n",
            "\n",
            "office  building      beneath athletics office\n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "sga aui ma  mail\n",
            "sga aui ma   \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Row 3:\n",
            "   \t\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "thank you for joining the waitlist to build with gpt      \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "to balance capacity with demand  we ll be sending invites gradually over time \n",
            "\n",
            " \n",
            "\n",
            "while we ramp up  invites will be prioritized to developers who have previously build with the openai api  you can also gain priority access if you contribute model evaluations to openai evals    that get merged  as this will help us improve the models for everyone \n",
            "\n",
            "\n",
            "once you re off the waitlist  you ll receive an email with further instructions on how to get started  we will process requests for the  k and   k models at different rates based on capacity  so you can expect to receive  k access first \n",
            "\n",
            "\n",
            "we appreciate your interest  and look forward to having you build with gpt   soon  in the meantime  we suggest getting started with gpt     turbo  the model powering chatgpt \n",
            "\n",
            "\n",
            "  the openai team\n",
            "\n",
            "\n",
            "p s  you can also preview gpt   on chat openai com    if you re a chatgpt plus subscriber  we expect to be severely capacity constrained  so there will be a usage cap for the model depending on demand and system performance \n",
            "\n",
            " \n",
            "\n",
            "openai      market st  pmb        san francisco  california        united states \n",
            "\n",
            "unsubscribe    manage preferences    \n",
            "\n",
            "   \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk -q"
      ],
      "metadata": {
        "id": "u8hw-8ZFqzBC"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the custom stopwords from the text file\n",
        "custom_stopwords = []\n",
        "with open('uni_stopwords.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        custom_stopwords.append(line.strip())\n",
        "\n",
        "# Combine NLTK's built-in stopwords with custom stopwords\n",
        "stopwords_list = stopwords.words('english') + custom_stopwords\n",
        "\n",
        "# Loop to remove stopwords, non-English words, and append the modified text\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "tokenized_data = []\n",
        "for row in data:\n",
        "    if 'Body' in row:\n",
        "        tokens = nltk.word_tokenize(row['Body'])\n",
        "        filtered_tokens = [word.lower() for word in tokens if (word.lower() not in stopwords_list and word.isalpha() and word.lower() in english_words)]\n",
        "        tokenized_data.append({'Body': filtered_tokens})\n",
        "    else:\n",
        "        tokenized_data.append(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlhMZNkIrEtc",
        "outputId": "be1bc7a1-afc4-451b-dca7-ea2940cc3922"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def print_top_25_words(data):\n",
        "    all_words = []\n",
        "    for row in data:\n",
        "        all_words.extend(row['Body'])\n",
        "    counter = Counter(all_words)\n",
        "    for word, count in counter.most_common(25):\n",
        "        print(word, count)\n",
        "\n",
        "print_top_25_words(tokenized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrcDyaVH-fMV",
        "outputId": "239f02e1-fd15-4a9a-faab-07daa1ccbab9"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "community 2858\n",
            "team 2589\n",
            "join 2499\n",
            "event 2395\n",
            "form 2367\n",
            "registration 2176\n",
            "session 2139\n",
            "aw 2113\n",
            "program 2025\n",
            "school 1974\n",
            "assignment 1973\n",
            "science 1945\n",
            "semester 1907\n",
            "meeting 1862\n",
            "club 1833\n",
            "today 1827\n",
            "week 1825\n",
            "room 1760\n",
            "professor 1702\n",
            "day 1628\n",
            "campus 1599\n",
            "project 1574\n",
            "available 1494\n",
            "model 1462\n",
            "business 1403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_data(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_data = []\n",
        "    for row in data:\n",
        "        if 'Body' in row:\n",
        "            tokens = row['Body']\n",
        "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "            lemmatized_data.append({'Body': lemmatized_tokens})\n",
        "        else:\n",
        "            lemmatized_data.append(row)\n",
        "    return lemmatized_data\n",
        "\n",
        "lemmatized_data = lemmatize_data(tokenized_data)"
      ],
      "metadata": {
        "id": "IntaMsYQtGs-"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_top_25_words(lemmatized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm5AaVaXBccp",
        "outputId": "33d080c7-cdd2-4066-b479-ff9fe8fbbd7f"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "community 2858\n",
            "session 2770\n",
            "team 2589\n",
            "join 2499\n",
            "day 2398\n",
            "event 2395\n",
            "form 2367\n",
            "registration 2176\n",
            "aw 2113\n",
            "program 2025\n",
            "school 1974\n",
            "assignment 1973\n",
            "class 1965\n",
            "science 1945\n",
            "semester 1907\n",
            "meeting 1862\n",
            "club 1833\n",
            "today 1827\n",
            "week 1825\n",
            "room 1760\n",
            "professor 1702\n",
            "campus 1599\n",
            "project 1574\n",
            "available 1494\n",
            "model 1462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(data, min_len=3, max_len=15):\n",
        "    filtered_data = []\n",
        "    for row in data:\n",
        "        if 'Body' in row:\n",
        "            tokens = row['Body']\n",
        "            filtered_tokens = [token for token in tokens if (len(token) >= min_len and len(token) <= max_len)]\n",
        "            filtered_data.append({'Body': filtered_tokens})\n",
        "        else:\n",
        "            filtered_data.append(row)\n",
        "    return filtered_data\n",
        "\n",
        "filtered_data = filter_data(lemmatized_data, min_len=3, max_len=15)"
      ],
      "metadata": {
        "id": "wNl3LruzBhjD"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_infrequent_words(data, min_freq=5):\n",
        "    all_words = []\n",
        "    for row in data:\n",
        "        if 'Body' in row:\n",
        "            tokens = row['Body']\n",
        "            all_words.extend(tokens)\n",
        "\n",
        "    word_freq = Counter(all_words)\n",
        "    filtered_data = []\n",
        "    for row in data:\n",
        "        if 'Body' in row:\n",
        "            tokens = row['Body']\n",
        "            filtered_tokens = [token for token in tokens if word_freq[token] >= min_freq]\n",
        "            filtered_data.append({'Body': filtered_tokens})\n",
        "        else:\n",
        "            filtered_data.append(row)\n",
        "    return filtered_data\n",
        "\n",
        "cleaned_data = remove_infrequent_words(filtered_data, min_freq=7)"
      ],
      "metadata": {
        "id": "QD_6zJdeDPIg"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the 'Body' value of the first three rows\n",
        "for i, row in enumerate(cleaned_data):\n",
        "    if i < 3:\n",
        "        print(f\"Row {i+1}:\")\n",
        "        print(row['Body'])\n",
        "        print()\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzLBy9orGJH6",
        "outputId": "6b0722f3-693e-4d03-888e-fb3bdcb25a81"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1:\n",
            "['daily', 'report', 'canvas', 'report', 'mar', 'submission', 'comment', 'project', 'engineering', 'visualization', 'comment', 'submission', 'project', 'notification']\n",
            "\n",
            "Row 2:\n",
            "['community', 'remind', 'back', 'hour', 'opportunity', 'wish', 'blessed', 'holy', 'month', 'bring', 'prosperity', 'health', 'peace', 'government', 'association', 'beneath', 'athletics']\n",
            "\n",
            "Row 3:\n",
            "['joining', 'build', 'balance', 'capacity', 'demand', 'sending', 'gradually', 'previously', 'build', 'gain', 'priority', 'access', 'contribute', 'model', 'improve', 'everyone', 'process', 'different', 'based', 'capacity', 'expect', 'access', 'appreciate', 'interest', 'look', 'build', 'soon', 'suggest', 'getting', 'turbo', 'model', 'team', 'preview', 'chat', 'plus', 'subscriber', 'expect', 'severely', 'capacity', 'usage', 'cap', 'model', 'depending', 'demand', 'system', 'performance', 'market', 'san', 'united', 'manage']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_top_25_words(cleaned_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDuD1g0QDSP2",
        "outputId": "0e51dad0-bbf1-44f2-c6a5-041564a2c958"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "community 2858\n",
            "session 2770\n",
            "team 2589\n",
            "join 2499\n",
            "day 2398\n",
            "event 2395\n",
            "form 2367\n",
            "registration 2176\n",
            "program 2025\n",
            "school 1974\n",
            "assignment 1973\n",
            "class 1965\n",
            "science 1945\n",
            "semester 1907\n",
            "meeting 1862\n",
            "club 1833\n",
            "today 1827\n",
            "week 1825\n",
            "room 1760\n",
            "professor 1702\n",
            "campus 1599\n",
            "project 1574\n",
            "available 1494\n",
            "model 1462\n",
            "business 1403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the tokenized data\n",
        "def join_tokens(data):\n",
        "    joined_data = []\n",
        "    for row in data:\n",
        "        if 'Body' in row:\n",
        "            tokens = row['Body']\n",
        "            joined_tokens = ' '.join(tokens)\n",
        "            joined_data.append({'Body': joined_tokens})\n",
        "        else:\n",
        "            joined_data.append(row)\n",
        "    return joined_data\n",
        "\n",
        "joined_data = join_tokens(cleaned_data)"
      ],
      "metadata": {
        "id": "Ge97qLd4D60h"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the 'Body' value of the first three rows\n",
        "for i, row in enumerate(joined_data):\n",
        "    if i < 3:\n",
        "        print(f\"Row {i+1}:\")\n",
        "        print(row['Body'])\n",
        "        print()\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddt-fCwnDexl",
        "outputId": "36215d98-e7b2-48cb-ff0b-555dc0042218"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1:\n",
            "daily report canvas report mar submission comment project engineering visualization comment submission project notification\n",
            "\n",
            "Row 2:\n",
            "community remind back hour opportunity wish blessed holy month bring prosperity health peace government association beneath athletics\n",
            "\n",
            "Row 3:\n",
            "joining build balance capacity demand sending gradually previously build gain priority access contribute model improve everyone process different based capacity expect access appreciate interest look build soon suggest getting turbo model team preview chat plus subscriber expect severely capacity usage cap model depending demand system performance market san united manage\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_bodies(data):\n",
        "    count = 0\n",
        "    for row in data:\n",
        "        if 'Body' in row:\n",
        "            count += 1\n",
        "    print(f'Total number of \"Body\" entries: {count}')\n",
        "\n",
        "count_bodies(joined_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE29lkWXEhOR",
        "outputId": "8fb014f4-f6d4-49d5-d09b-9216164a8419"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of \"Body\" entries: 12294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -q"
      ],
      "metadata": {
        "id": "EHFZzwLSEqfh"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks the joined_data list for \"<number>\" in the \"To: (Name)\" field, and classifies each email as personal or not\n",
        "def classify_personal(data):\n",
        "    personal_data = []\n",
        "    number_regex = re.compile('< \\s*(\\d+)\\s* >')\n",
        "    for row in data:\n",
        "        if 'To: (Name)' in row:\n",
        "            to_field = row['To: (Name)']\n",
        "            to_field = number_regex.sub(r'\\d+', to_field)\n",
        "            row['is_personal'] = bool(re.search('^\\d+@', to_field))\n",
        "        else:\n",
        "            row['is_personal'] = False\n",
        "        personal_data.append(row)\n",
        "    return personal_data\n",
        "\n",
        "personal_data = classify_personal(joined_data)"
      ],
      "metadata": {
        "id": "k-ZUzJvQFOo2"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create a list of the known topics\n",
        "known_topics = ['event', 'workshop', 'club', 'professional', 'jobs & internship', 'general']\n",
        "\n",
        "# Create a dictionary mapping known topics to a list of keywords\n",
        "topic_keywords = {\n",
        "    'event': ['party', 'gathering', 'conference', 'event'],\n",
        "    'workshop': ['training', 'course', 'seminar', 'workshop'],\n",
        "    'club': ['group', 'organization', 'association', 'club', 'meeting'],\n",
        "    'professional': ['networking', 'development','grading', 'graded', 'assignment', 'grade'],\n",
        "    'jobs & internship': ['job', 'internship', 'employment', 'career'],\n",
        "    'general': ['information', 'news', 'announcement']\n",
        "}\n",
        "\n",
        "# Create a list of documents from the 'Body' entries in the personal_data\n",
        "documents = [row['Body'] for row in personal_data if 'Body' in row and not row['is_personal']]\n",
        "\n",
        "def find_topic_by_keyword(body, topic_keywords):\n",
        "    for topic, keywords in topic_keywords.items():\n",
        "        if any(keyword in body for keyword in keywords):\n",
        "            return topic\n",
        "    return None\n",
        "\n",
        "mapped_documents = []\n",
        "unmapped_documents = []\n",
        "\n",
        "\n",
        "# Map documents to topics using keywords\n",
        "for document in documents:\n",
        "    topic = find_topic_by_keyword(document, topic_keywords)\n",
        "    if topic is not None:\n",
        "        mapped_documents.append((document, topic))\n",
        "    else:\n",
        "        unmapped_documents.append(document)\n",
        "\n",
        "\n",
        "# Train LDA and NMF models on unmapped documents\n",
        "unmapped_documents = [doc for doc, _ in mapped_documents]\n",
        "\n",
        "# Tokenize each document\n",
        "tokenized_documents = [doc.split() for doc in unmapped_documents]\n",
        "\n",
        "# Create a dictionary from the tokenized documents\n",
        "dictionary = corpora.Dictionary(tokenized_documents)\n",
        "\n",
        "# Create a bag-of-words representation of the documents\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]"
      ],
      "metadata": {
        "id": "r3i1pECFI6hP"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an LDA model on the bag-of-words corpus\n",
        "num_topics = len(known_topics)\n",
        "lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)"
      ],
      "metadata": {
        "id": "3nzCGjukcF-4"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Compute LDA model coherence score\n",
        "lda_coherence_model = CoherenceModel(model=lda_model, texts=tokenized_documents, dictionary=dictionary, coherence='c_v')\n",
        "lda_coherence_score = lda_coherence_model.get_coherence()\n",
        "\n",
        "print(f\"LDA Coherence Score: {lda_coherence_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGYRlCM4gzui",
        "outputId": "faf398ad-5187-4f9a-861d-9c34560046f2"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA Coherence Score: 0.4492731103083732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an NMF model on the TF-IDF representation of the documents\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(unmapped_documents)\n",
        "nmf_model = NMF(n_components=num_topics, random_state=1).fit(tfidf_matrix)"
      ],
      "metadata": {
        "id": "wBYr372fcHk6"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nmf_top_words(nmf_model, vectorizer, num_words=10):\n",
        "    topic_words = []\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
        "        top_words_indices = topic.argsort()[:-num_words - 1:-1]\n",
        "        top_words = [feature_names[i] for i in top_words_indices]\n",
        "        topic_words.append(top_words)\n",
        "        \n",
        "    return topic_words\n",
        "\n",
        "nmf_top_words = get_nmf_top_words(nmf_model, vectorizer)\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Compute NMF model coherence score\n",
        "nmf_coherence_model = CoherenceModel(topics=nmf_top_words, texts=tokenized_documents, dictionary=dictionary, coherence='c_v')\n",
        "nmf_coherence_score = nmf_coherence_model.get_coherence()\n",
        "\n",
        "print(f\"NMF Coherence Score: {nmf_coherence_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEBN_trwg5P7",
        "outputId": "088624d6-ab1a-4d51-964b-6a98bc2463c2"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMF Coherence Score: 0.6876887876173748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corextopic -q"
      ],
      "metadata": {
        "id": "mzFJVqW0kOmL"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from corextopic import corextopic as ct\n",
        "\n",
        "# Convert the tokenized documents back to strings\n",
        "preprocessed_documents = [' '.join(doc) for doc in tokenized_documents]\n",
        "\n",
        "# Vectorize the preprocessed data using TfidfVectorizer\n",
        "vectorizer1 = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "tfidf_matrix1 = vectorizer1.fit_transform(preprocessed_documents)\n",
        "\n",
        "# Now that the vectorizer is fitted, you can get the feature names\n",
        "feature_names = vectorizer1.get_feature_names_out()\n",
        "\n",
        "# Create a CorEx model with your predefined topics\n",
        "num_topics = len(known_topics)\n",
        "corex_model = ct.Corex(n_hidden=num_topics, words=feature_names, max_iter=100, verbose=False, seed=42)\n",
        "\n",
        "# Fit the CorEx model to the vectorized data\n",
        "corex_model.fit(tfidf_matrix1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECnj1ZSAhCFX",
        "outputId": "ccf41db1-39fb-4d75-e7cf-d91e1ece2f11"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<corextopic.corextopic.Corex at 0x7ff1e02d3130>"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top words for each topic\n",
        "def get_corex_top_words(corex_model, feature_names, num_words=10):\n",
        "    topic_words = []\n",
        "    for topic_idx, topic in enumerate(corex_model.get_topics()):\n",
        "        top_words = [feature_names[word_idx] for word_idx, _, _ in topic[:num_words]]\n",
        "        topic_words.append(top_words)\n",
        "    return topic_words\n",
        "\n",
        "corex_top_words = get_corex_top_words(corex_model, feature_names)\n",
        "\n",
        "# Calculate the coherence score using Gensim's CoherenceModel\n",
        "corex_coherence_model = CoherenceModel(topics=corex_top_words, texts=tokenized_documents, dictionary=dictionary, coherence='c_v')\n",
        "corex_coherence_score = corex_coherence_model.get_coherence()\n",
        "\n",
        "print(f\"CorEx Coherence Score: {corex_coherence_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--UUfK3SkuRH",
        "outputId": "46282f07-b4b5-4083-8bca-e882b063ce3c"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\n",
            "CorEx Coherence Score: 0.5516474376788097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy -q"
      ],
      "metadata": {
        "id": "y3xs74JIdzPG"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the top words for each topic from the LDA model\n",
        "def get_lda_top_words(lda_model, num_words=10):\n",
        "    topic_words = []\n",
        "    for topic_id in range(lda_model.num_topics):\n",
        "        words = lda_model.show_topic(topic_id, topn=num_words)\n",
        "        topic_words.append([w[0] for w in words])\n",
        "    return topic_words\n",
        "\n",
        "lda_top_words = get_lda_top_words(lda_model, num_words=10)\n",
        "\n",
        "# Get the top words for each topic from the NMF model\n",
        "def get_nmf_top_words(nmf_model, vectorizer, num_words=10):\n",
        "    topic_words = []\n",
        "    for topic_id in range(nmf_model.components_.shape[0]):\n",
        "        word_ids = nmf_model.components_[topic_id].argsort()[:-num_words - 1:-1]\n",
        "        topic_words.append([vectorizer.get_feature_names_out()[i] for i in word_ids])\n",
        "    return topic_words\n",
        "\n",
        "nmf_top_words = get_nmf_top_words(nmf_model, vectorizer)\n",
        "\n",
        "# Combine the top words from LDA, NMF, and CorEx models for each topic\n",
        "combined_top_words = []\n",
        "for i, topic in enumerate(known_topics):\n",
        "    print(f\"Topic: {topic}\")\n",
        "    print(\"Top words from LDA model:\")\n",
        "    print(lda_top_words[i])\n",
        "    print(\"Top words from NMF model:\")\n",
        "    print(nmf_top_words[i])\n",
        "    print(\"Top words from CorEx model:\")\n",
        "    print(corex_top_words[i])\n",
        "    combined_words = list(set(lda_top_words[i] + nmf_top_words[i] + corex_top_words[i]))\n",
        "    combined_top_words.append(combined_words)\n",
        "    print(\"Combined top words:\")\n",
        "    print(combined_words)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhVvOvigckUq",
        "outputId": "6f5297b7-b509-4ba5-82e5-74ebf187ed32"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: event\n",
            "Top words from LDA model:\n",
            "['club', 'event', 'community', 'professor', 'science', 'meeting', 'place', 'join', 'semester', 'tomorrow']\n",
            "Top words from NMF model:\n",
            "['assignment', 'graded', 'review', 'notification', 'roll', 'call', 'attendance', 'grade', 'quiz', 'mar']\n",
            "Top words from CorEx model:\n",
            "['assignment', 'graded', 'notification', 'review', 'roll', 'caution', 'grade', 'attendance', 'quiz', 'exercise']\n",
            "Combined top words:\n",
            "['notification', 'assignment', 'semester', 'meeting', 'call', 'tomorrow', 'mar', 'join', 'grade', 'club', 'event', 'caution', 'science', 'place', 'professor', 'exercise', 'attendance', 'graded', 'community', 'quiz', 'roll', 'review']\n",
            "\n",
            "\n",
            "Topic: workshop\n",
            "Top words from LDA model:\n",
            "['model', 'layer', 'registration', 'train', 'size', 'input', 'test', 'intermediate', 'history', 'rate']\n",
            "Top words from NMF model:\n",
            "['event', 'club', 'community', 'session', 'program', 'research', 'place', 'team', 'semester', 'sao']\n",
            "Top words from CorEx model:\n",
            "['enroll', 'machine', 'skywards', 'sap', 'reserved', 'cloud', 'earn', 'model', 'headquarters', 'specialization']\n",
            "Combined top words:\n",
            "['sap', 'specialization', 'registration', 'skywards', 'team', 'history', 'earn', 'rate', 'intermediate', 'semester', 'train', 'research', 'session', 'model', 'sao', 'club', 'event', 'test', 'layer', 'program', 'headquarters', 'size', 'place', 'input', 'reserved', 'community', 'cloud', 'machine', 'enroll']\n",
            "\n",
            "\n",
            "Topic: club\n",
            "Top words from LDA model:\n",
            "['assignment', 'graded', 'due', 'notification', 'review', 'report', 'school', 'group', 'international', 'social']\n",
            "Top words from NMF model:\n",
            "['meeting', 'join', 'mobile', 'computer', 'device', 'web', 'room', 'science', 'professor', 'schedule']\n",
            "Top words from CorEx model:\n",
            "['thesis', 'master', 'reader', 'examination', 'partial', 'composed', 'defend', 'fulfillment', 'external', 'examiner']\n",
            "Combined top words:\n",
            "['web', 'notification', 'report', 'room', 'reader', 'assignment', 'international', 'meeting', 'defend', 'master', 'social', 'device', 'fulfillment', 'examiner', 'thesis', 'school', 'join', 'composed', 'examination', 'external', 'schedule', 'due', 'group', 'computer', 'science', 'professor', 'graded', 'partial', 'mobile', 'review']\n",
            "\n",
            "\n",
            "Topic: professional\n",
            "Top words from LDA model:\n",
            "['meeting', 'join', 'housing', 'form', 'book', 'campus', 'group', 'cash', 'trip', 'day']\n",
            "Top words from NMF model:\n",
            "['caution', 'exercise', 'unknown', 'opening', 'outside', 'link', 'organization', 'especially', 'slack', 'privacy']\n",
            "Top words from CorEx model:\n",
            "['research', 'empowerment', 'faculty', 'informal', 'expose', 'gender', 'facilitate', 'bird', 'abstract', 'engage']\n",
            "Combined top words:\n",
            "['abstract', 'cash', 'housing', 'day', 'privacy', 'empowerment', 'engage', 'expose', 'meeting', 'book', 'research', 'gender', 'trip', 'opening', 'unknown', 'faculty', 'form', 'slack', 'join', 'outside', 'especially', 'caution', 'facilitate', 'link', 'group', 'campus', 'exercise', 'organization', 'bird', 'informal']\n",
            "\n",
            "\n",
            "Topic: jobs & internship\n",
            "Top words from LDA model:\n",
            "['read', 'machine', 'caution', 'organization', 'model', 'training', 'team', 'text', 'language', 'cloud']\n",
            "Top words from NMF model:\n",
            "['thesis', 'master', 'reader', 'digital', 'communication', 'medium', 'announcement', 'examination', 'partial', 'composed']\n",
            "Top words from CorEx model:\n",
            "['canada', 'surfaced', 'promotional', 'visit', 'employee', 'ago', 'mandatory', 'helpful', 'authorized', 'intended']\n",
            "Combined top words:\n",
            "['mandatory', 'team', 'canada', 'authorized', 'text', 'surfaced', 'ago', 'reader', 'visit', 'partial', 'training', 'master', 'communication', 'read', 'promotional', 'thesis', 'composed', 'intended', 'model', 'medium', 'examination', 'caution', 'digital', 'employee', 'language', 'cloud', 'organization', 'machine', 'helpful', 'announcement']\n",
            "\n",
            "\n",
            "Topic: general\n",
            "Top words from LDA model:\n",
            "['session', 'team', 'program', 'development', 'explorer', 'workshop', 'event', 'meeting', 'join', 'career']\n",
            "Top words from NMF model:\n",
            "['due', 'report', 'assignment', 'date', 'science', 'engineering', 'canvas', 'ending', 'weekly', 'professor']\n",
            "Top words from CorEx model:\n",
            "['criticism', 'prairie', 'duck', 'lac', 'brother', 'pond', 'radicalization', 'nous', 'sence', 'tait']\n",
            "Combined top words:\n",
            "['lac', 'tait', 'brother', 'report', 'team', 'career', 'ending', 'development', 'assignment', 'workshop', 'duck', 'meeting', 'radicalization', 'session', 'canvas', 'join', 'weekly', 'event', 'prairie', 'engineering', 'explorer', 'pond', 'date', 'program', 'due', 'science', 'professor', 'nous', 'sence', 'criticism']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPAmurYdgkcG"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics and the top words in each topic\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    topic_name = known_topics[idx]\n",
        "    print(f'{topic_name}: {topic}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36nvK2weI_GE",
        "outputId": "717b9c02-1fe3-4964-c0fd-fea54fa319d9"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "event: 0.023*\"club\" + 0.022*\"event\" + 0.014*\"community\" + 0.010*\"professor\" + 0.010*\"science\" + 0.010*\"meeting\" + 0.009*\"place\" + 0.009*\"join\" + 0.008*\"semester\" + 0.008*\"tomorrow\"\n",
            "workshop: 0.026*\"model\" + 0.024*\"layer\" + 0.024*\"registration\" + 0.023*\"train\" + 0.022*\"size\" + 0.021*\"input\" + 0.021*\"test\" + 0.018*\"intermediate\" + 0.017*\"history\" + 0.016*\"rate\"\n",
            "club: 0.051*\"assignment\" + 0.027*\"graded\" + 0.024*\"due\" + 0.021*\"notification\" + 0.018*\"review\" + 0.015*\"report\" + 0.014*\"school\" + 0.013*\"group\" + 0.012*\"international\" + 0.011*\"social\"\n",
            "professional: 0.013*\"meeting\" + 0.010*\"join\" + 0.009*\"housing\" + 0.009*\"form\" + 0.008*\"book\" + 0.007*\"campus\" + 0.007*\"group\" + 0.007*\"cash\" + 0.006*\"trip\" + 0.006*\"day\"\n",
            "jobs & internship: 0.007*\"read\" + 0.007*\"machine\" + 0.007*\"caution\" + 0.006*\"organization\" + 0.006*\"model\" + 0.006*\"training\" + 0.005*\"team\" + 0.005*\"text\" + 0.005*\"language\" + 0.005*\"cloud\"\n",
            "general: 0.017*\"session\" + 0.011*\"team\" + 0.010*\"program\" + 0.008*\"development\" + 0.008*\"explorer\" + 0.007*\"workshop\" + 0.006*\"event\" + 0.006*\"meeting\" + 0.006*\"join\" + 0.005*\"career\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = 'Emails.json'\n",
        "\n",
        "output_file = 'Email_add.json'\n",
        "\n",
        "# Read the JSON file and store the email bodies in a list\n",
        "with open(json_file, mode='r', encoding='utf-8') as f:\n",
        "    data1 = json.load(f)\n",
        "\n",
        "original_documents = [row['Body'] for row in data1 if 'Body' in row]\n",
        "\n",
        "# Map the documents to the known topics\n",
        "for i, document in enumerate(tokenized_documents):\n",
        "    bow_vector = dictionary.doc2bow(document)\n",
        "    topic_scores = lda_model[bow_vector]\n",
        "    top_topic = max(topic_scores, key=lambda x: x[1])[0] + 1\n",
        "    topic_name = known_topics[top_topic-1]\n",
        "\n",
        "    # Add the topic and personal fields to the email data\n",
        "    data1[i]['Category'] = topic_name\n",
        "    data1[i]['Is_personal'] = bool(topic_name == 'personal')\n",
        "\n",
        "    # Print if the top topic is 'personal'\n",
        "    if topic_name == 'personal':\n",
        "        print(f'Document {i+1}: {original_documents[i]}')\n",
        "        print(\"The topic is personal.\")\n",
        "\n",
        "# Write the updated email data to a new JSON file\n",
        "with open(output_file, mode='w', encoding='utf-8') as f:\n",
        "    json.dump(data1, f, indent=2)"
      ],
      "metadata": {
        "id": "UjcPP7cKM78S"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump((lda_model, nmf_model, corex_model), f)"
      ],
      "metadata": {
        "id": "z4dzWmufuheJ"
      },
      "execution_count": 269,
      "outputs": []
    }
  ]
}